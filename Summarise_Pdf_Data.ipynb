{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gk58IISERTvm98/RAG-Application/blob/main/Extract_Data_from_Pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[all-docs]\""
      ],
      "metadata": {
        "id": "LAJmLomYDVxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic lxml\n"
      ],
      "metadata": {
        "id": "rXBeFImEZqhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp3BhfCvQl7a",
        "outputId": "5367780e-5504-4e7d-9b62-3c91b20f912f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "id": "zgi7V-S4DV21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get install libmagic-dev\n",
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "r5Wi1wy2RO5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libleptonica-dev libtesseract-dev tesseract-ocr-eng tesseract-ocr-script-latn\n"
      ],
      "metadata": {
        "id": "Mnnp2zBDDV-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured-pytesseract"
      ],
      "metadata": {
        "id": "cO-8dY0CDWAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tesseract-ocr"
      ],
      "metadata": {
        "id": "5uiYLv9qfEEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk\n"
      ],
      "metadata": {
        "id": "ZhKoy4-cpWh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "kBfYDnUDIv5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.data.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4ZvPXaiom8z",
        "outputId": "4dbcf507-0e28-497f-f89e-14959b0d5f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf"
      ],
      "metadata": {
        "id": "VGd4HjYJDWGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements= partition_pdf(\n",
        "    filename=\"/content/ragmodel.pdf\",\n",
        "    strategy= \"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types= [\"Image\", \"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data2\"\n",
        "    )"
      ],
      "metadata": {
        "id": "uRrj0ZJ_IANX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements"
      ],
      "metadata": {
        "id": "RnsxoQ_SgPQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Header=[]\n",
        "Footer=[]\n",
        "Title=[]\n",
        "NarrativeText=[]\n",
        "Text=[]\n",
        "ListItem=[]\n",
        "\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
        "            Header.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
        "            Footer.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "            Title.append(str(element))\n",
        "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
        "            Text.append(str(element))\n",
        "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem.append(str(element))"
      ],
      "metadata": {
        "id": "Z-J24auGesfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma langchain-experimental"
      ],
      "metadata": {
        "id": "3kzeO-tqIFeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core\n",
        "!pip install langchain_openai"
      ],
      "metadata": {
        "id": "zcO0rqpKesxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "bN7hjeHzsJVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "OPENAI_API_TOKEN=userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
      ],
      "metadata": {
        "id": "ugwDU_kGsZz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table or Text Summary"
      ],
      "metadata": {
        "id": "2sM9oOTQwe5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tables = []\n",
        "texts = []\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "    tables.append(str(element))\n",
        "  elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "    texts.append(str(element))\n",
        "\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=4000, chunk_overlap=0\n",
        ")\n",
        "joined_texts = \" \".join(texts)\n",
        "texts_4k_token = text_splitter.split_text(joined_texts)\n"
      ],
      "metadata": {
        "id": "Zc_cL9uX1yxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
        "  # Prompt\n",
        "  prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "  These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "  Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
        "  prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "  # Text summary chain\n",
        "  model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "  summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "  # Initialize empty summaries\n",
        "  text_summaries = []\n",
        "  table_summaries = []\n",
        "  # Apply to text if texts are provided and summarization is requested\n",
        "  if texts and summarize_texts:\n",
        "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
        "  elif texts:\n",
        "    text_summaries = texts\n",
        "\n",
        "  # Apply to tables if tables are provided\n",
        "  if tables:\n",
        "    table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
        "\n",
        "  return text_summaries, table_summaries\n",
        "\n"
      ],
      "metadata": {
        "id": "LZEs8j6D0rk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get text, table summaries\n",
        "text_summaries, table_summaries = generate_text_summaries(\n",
        "    texts_4k_token, tables, summarize_texts=True\n",
        ")"
      ],
      "metadata": {
        "id": "BWV6xABZFe_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries[0]"
      ],
      "metadata": {
        "id": "F4Vw_y6RuDrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Summary"
      ],
      "metadata": {
        "id": "JyJmR-A6wOmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "from langchain_core.messages import HumanMessage\n"
      ],
      "metadata": {
        "id": "jvsviCbQuJON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_image(image_path):\n",
        "  \"\"\"Getting the base64 string\"\"\"\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "VeS4xkHxuJRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def image_summarize(img_base64, prompt):\n",
        "  \"\"\"Make image summary\"\"\"\n",
        "  chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
        "  msg = chat.invoke(\n",
        "      [\n",
        "          HumanMessage(\n",
        "              content=[\n",
        "                  {\"type\": \"text\", \"text\": prompt},\n",
        "                   {\n",
        "                      \"type\": \"image_url\",\n",
        "                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
        "                  },\n",
        "              ]\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "  return msg.content"
      ],
      "metadata": {
        "id": "QdNuNEADuJUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_img_summaries(path):\n",
        "  img_base64_list = []\n",
        "  # Store image summaries\n",
        "  image_summaries = []\n",
        "  # Prompt\n",
        "  prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "  These summaries will be embedded and used to retrieve the raw image. \\\n",
        "  Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "  # Apply to images\n",
        "  for img_file in sorted(os.listdir(path)):\n",
        "    if img_file.endswith(\".jpg\"):\n",
        "      img_path = os.path.join(path, img_file)\n",
        "      base64_image = encode_image(img_path)\n",
        "      img_base64_list.append(base64_image)\n",
        "      image_summaries.append(image_summarize(base64_image, prompt))\n",
        "\n",
        "  return img_base64_list, image_summaries"
      ],
      "metadata": {
        "id": "1z_3mztkuJX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpath=\"/content/extracted_data2/figure-17-4.jpg\"\n",
        "\n",
        "img_base64_list,image_summaries=generate_img_summaries(fpath)"
      ],
      "metadata": {
        "id": "gvy8Qm9QuJhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries[0]"
      ],
      "metadata": {
        "id": "jbZZyE-JunQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Vector Retriever"
      ],
      "metadata": {
        "id": "r0kj05JoHuib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "def create_multi_vector_retriever(\n",
        "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
        "):\n",
        "  # Initialize the storage layer\n",
        "  store = InMemoryStore()\n",
        "  id_key = \"doc_id\"\n",
        "\n",
        "  # Create the multi-vector retriever\n",
        "  retriever = MultiVectorRetriever(\n",
        "      vectorstore=vectorstore,\n",
        "      docstore=store,\n",
        "      id_key=id_key,\n",
        "  )\n",
        "\n",
        "  # Helper function to add documents to the vectorstore and docstore\n",
        "  def add_documents(retriever, doc_summaries, doc_contents):\n",
        "    doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "    summary_docs = [\n",
        "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "        for i, s in enumerate(doc_summaries)\n",
        "    ]\n",
        "    retriever.vectorstore.add_documents(summary_docs)\n",
        "    retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "  # Add texts, tables, and images\n",
        "  # Check that text_summaries is not empty before adding\n",
        "  if text_summaries:\n",
        "    add_documents(retriever, text_summaries, texts)\n",
        "  # Check that table_summaries is not empty before adding\n",
        "  if table_summaries:\n",
        "    add_documents(retriever, table_summaries, tables)\n",
        "  # Check that image_summaries is not empty before adding\n",
        "  if image_summaries:\n",
        "    add_documents(retriever, image_summaries, images)\n",
        "\n",
        "  return retriever\n"
      ],
      "metadata": {
        "id": "wDq45XlOHKyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma(\n",
        "    collection_name=\"mm_rag_blog\", embedding_function=OpenAIEmbeddings()\n",
        ")"
      ],
      "metadata": {
        "id": "jHwRhqxCHZvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create retriever\n",
        "retriever_multi_vector_img = create_multi_vector_retriever(\n",
        "    vectorstore,\n",
        "    text_summaries,\n",
        "    texts,\n",
        "    table_summaries,\n",
        "    tables,\n",
        "    image_summaries,\n",
        "    img_base64_list,\n",
        ")"
      ],
      "metadata": {
        "id": "7-qswSXEHcK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG\n",
        "\n",
        "**Build retriever**\\\n",
        "We need to bin the retrieved doc(s) into the correct parts of the GPT-4V prompt template."
      ],
      "metadata": {
        "id": "FIU6Ao5Bw4KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "azt6cz8VI0ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_img_base64(img_base64):\n",
        "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
        "    # Create an HTML img tag with the base64 string as the source\n",
        "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
        "    # Display the image by rendering the HTML\n",
        "    display(HTML(image_html))\n"
      ],
      "metadata": {
        "id": "Ud6ZoiEHI0iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def looks_like_base64(sb):\n",
        "    \"\"\"Check if the string looks like base64\"\"\"\n",
        "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n"
      ],
      "metadata": {
        "id": "czezIT3KI0l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def is_image_data(b64data):\n",
        "  \"\"\"\n",
        "  Check if the base64 data is an image by looking at the start of the data\n",
        "  \"\"\"\n",
        "  image_signatures = {\n",
        "      b\"\\xff\\xd8\\xff\": \"jpg\",\n",
        "      b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
        "      b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "      b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "  }\n",
        "  try:\n",
        "      header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
        "      for sig, format in image_signatures.items():\n",
        "          if header.startswith(sig):\n",
        "              return True\n",
        "      return False\n",
        "  except Exception:\n",
        "      return False"
      ],
      "metadata": {
        "id": "71KkZWsAJHLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_base64_image(base64_string, size=(128, 128)):\n",
        "  img_data = base64.b64decode(base64_string)\n",
        "  img = Image.open(io.BytesIO(img_data))\n",
        "  # Resize the image\n",
        "  resized_img = img.resize(size, Image.LANCZOS)\n",
        "  # Save the resized image to a bytes buffer\n",
        "  buffered = io.BytesIO()\n",
        "  resized_img.save(buffered, format=img.format)\n",
        "  # Encode the resized image to Base64\n",
        "  return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n"
      ],
      "metadata": {
        "id": "OxnJODDhJHOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_image_text_types(docs):\n",
        "  \"\"\"\n",
        "  Split base64-encoded images and texts\n",
        "  \"\"\"\n",
        "  b64_images = []\n",
        "  texts = []\n",
        "  for doc in docs:\n",
        "    # Check if the document is of type Document and extract page_content if so\n",
        "    if isinstance(doc, Document):\n",
        "        doc = doc.page_content\n",
        "    if looks_like_base64(doc) and is_image_data(doc):\n",
        "        doc = resize_base64_image(doc, size=(1300, 600))\n",
        "        b64_images.append(doc)\n",
        "    else:\n",
        "        texts.append(doc)\n",
        "  return {\"images\": b64_images, \"texts\": texts}"
      ],
      "metadata": {
        "id": "xl54D_rMJLJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_prompt_func(data_dict):\n",
        "    \"\"\"\n",
        "    Join the context into a single string\n",
        "    \"\"\"\n",
        "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "    messages = []\n",
        "\n",
        "    # Adding image(s) to the messages if present\n",
        "    if data_dict[\"context\"][\"images\"]:\n",
        "        for image in data_dict[\"context\"][\"images\"]:\n",
        "            image_message = {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "            }\n",
        "            messages.append(image_message)\n",
        "\n",
        "    # Adding the text for analysis\n",
        "    text_message = {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": (\n",
        "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
        "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
        "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
        "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "            \"Text and / or tables:\\n\"\n",
        "            f\"{formatted_texts}\"\n",
        "        ),\n",
        "    }\n",
        "    messages.append(text_message)\n",
        "    return [HumanMessage(content=messages)]"
      ],
      "metadata": {
        "id": "AAzRyUaJJLNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_rag_chain(retriever):\n",
        "    \"\"\"\n",
        "    Multi-modal RAG chain\n",
        "    \"\"\"\n",
        "\n",
        "    # Multi-modal LLM\n",
        "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
        "\n",
        "    # RAG pipeline\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "        | RunnableLambda(img_prompt_func)\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return chain\n"
      ],
      "metadata": {
        "id": "3PjpFqklJLSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
      ],
      "metadata": {
        "id": "AeP2x7tVJHS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check retrieval\n",
        "query = \"What are the EV / NTM and NTM rev growth for MongoDB, Cloudflare, and Datadog?\"\n",
        "docs = retriever_multi_vector_img.invoke(query, limit=6)\n",
        "\n",
        "# We get 4 docs\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "JI_DSIkjJvsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get back relevant images\n",
        "plt_img_base64(docs[0])"
      ],
      "metadata": {
        "id": "BBGjwJk_Jvyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries[3]"
      ],
      "metadata": {
        "id": "qbxgTZr9KOUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's run RAG and test the ability to synthesize an answer to our question.**"
      ],
      "metadata": {
        "id": "Nl8vFVIoKYaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RAG chain\n",
        "chain_multimodal_rag.invoke(query)"
      ],
      "metadata": {
        "id": "b-diQEowJv24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISsZzJUyKfN2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
